视频地址：https://ke.qq.com/webcourse/index.html#course_id=254278&term_id=100299829&taid=1561426770780486&vid=q1422up50wp
自然语言处理：snowNLP
pip install snownlp
NLP: 分词，标记(词性，名词、介词、形容词、动词等），提取
应用场景：情感分析 snowNLP('XXXXXXX').sentiments  舆情分析
------------------------------------------------
NLTK 工业级NLP库 词库列表：www.nltk.org/nltk_data  书籍列表：www.nltk.org/book
用于：分类、分词、标签、提取、解析、语义推理
pip install nltk
nltk.download()
nltk.word_tokenize('xxxxx')  分词 
nltk.pos_tag('分词')  打标签、标签
nltk.chunk.ne_chunk('标签')  分类串成树
nltk.corpus.brown.words() 语料库
text.similar('xxx') 相近词

--------------------------------------------------
名字预测男女
nltk.corpus.names.words('文件名') 引入文件所有男女人名，特征提取
random.shuffle(tuple) 对字符串进行打乱，随机
训练：nltk.NaiveBayesClassifier.train(xxxx)
测试：[1 if classifier.classify(features) == gender else 0 for (features, gender) in test_set] 推导式，三元表达式
特征： 最后一个字母+第一个字母

--------------------------------------------------------
jieba 分词
www.oschina.net/p/jieba
pip install jieba
jieba.cut('xxxx') 中文分词 cut_all = true 所有可能的组词都切出来
posseg.cut() 切词加语法
NLTK + jieba
tags = [(items.word , items.flag) for items in list(jieba.posseg.cut('xxx'))]  中文语法切词
nltk.chunk.ne_chunk(tags)  对语法分类串树

---------------------------------------------------------
新闻分类：
1、先找到代表性的词 ——————最常出现的N个词
2、

os.listdir(os.getcwd()+'/xxx/') 读取文件目录
open(os.getcwd()+path+'/'+filename , 'r', encoding='utf-8') 读取文件内容
d = nltk.probability.FreqDist(jieba.cut(content)) 对单词出现评率进行统计
l = sorted(list(d.items()) , key = lambda x : x[1] , reverse = true)[:10]  对单词出现频率进行排序,取前十个

words = [w for (w , c) in l]
提取特征函数：
def getFeature(content):
result = {}
for w in words:
  resule[w] = 0
l = list(jieba.cut(content))
for w in l :
  if w in words :
    result[w] += 1
return result






